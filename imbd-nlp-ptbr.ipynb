{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":95829,"sourceType":"datasetVersion","datasetId":51413}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.idle":"2025-03-24T17:49:23.688957Z","shell.execute_reply.started":"2025-03-24T17:49:23.240736Z","shell.execute_reply":"2025-03-24T17:49:23.687730Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-ptbr/imdb-reviews-pt-br.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports do Projeto","metadata":{}},{"cell_type":"markdown","source":"## Ajuste para trabalhar com TPU do kaggle","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade torch_xla\n!pip install --upgrade torch\n!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:49:23.689728Z","iopub.execute_input":"2025-03-24T17:49:23.690003Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_xla in /usr/local/lib/python3.10/site-packages (2.6.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torch_xla) (2.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from torch_xla) (2.32.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from torch_xla) (6.0.2)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch_xla) (2.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla) (2.3.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla) (3.4.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->torch_xla) (2025.1.31)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.5.0)\nCollecting torch\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m632.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch) (11.2.1.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.4.2)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch) (11.6.1.9)\nCollecting nvidia-cusparselt-cu12==0.6.2\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2025.2.0)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.17.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch) (12.3.1.170)\nCollecting triton==3.2.0\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch) (2.21.5)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch) (1.13.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.5)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\nInstalling collected packages: triton, nvidia-cusparselt-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.0\n    Uninstalling torch-2.5.0:\n      Successfully uninstalled torch-2.5.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.20.0 requires torch==2.5.0, but you have torch 2.6.0 which is incompatible.\ntorchaudio 2.5.0 requires torch==2.5.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cusparselt-cu12-0.6.2 torch-2.6.0 triton-3.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.50.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.5.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch_xla\nimport torch_xla.core.xla_model as xm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports Modelo","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport time\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pr√©-processamento dos dados","metadata":{}},{"cell_type":"code","source":"# Defini√ßao de semente aleatoria para consistencia\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_train_df = pd.read_csv('/kaggle/input/imdb-ptbr/imdb-reviews-pt-br.csv')\nreviews_train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Converter os valores da coluna 'sentiment' para bin√°rio\nreviews_train_df['sentiment'] = reviews_train_df['sentiment'].map({'neg': 0, 'pos': 1})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ajustando df para colunas que importam\nreviews_train_df_novo = reviews_train_df[['text_pt', 'sentiment']]\nreviews_train_df_novo.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remo√ßao de palavras sem sentido semantico utilizando o spaCy, para manter o contexto\n\n!pip install spacy\n!python -m spacy download pt_core_news_sm\n\nimport spacy\n\nnlp = spacy.load(\"pt_core_news_sm\")\n\ndef preprocess_text(text):\n    text = text.lower()\n\n    doc = nlp(text)\n\n    cleaned_tokens = [\n        token.lemma_ for token in doc\n        if not token.is_punct\n        and not token.is_space\n        and not token.is_stop\n        and not token.like_url\n        and not token.like_email\n        and token.is_alpha\n        and len(token.text) > 2\n    ]\n\n    cleaned_text = \" \".join(cleaned_tokens)\n    return cleaned_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_train_df_novo['text_pt_processed'] = reviews_train_df_novo['text_pt'].apply(preprocess_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_train_df_novo.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Salvar o DataFrame processado\nreviews_train_df_novo.to_csv('/kaggle/input/imdb-ptbr/reviews_train_df_novo.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reviews_train_df_novo = reviews_train_df_novo.drop('text_pt', axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ajustando ordens da coluna (por puro toc)\nreviews_train_df_novo = reviews_train_df_novo[['text_pt_processed', 'sentiment']]\nreviews_train_df_novo.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Treinos e testes","metadata":{}},{"cell_type":"code","source":"# Tokeniza√ßao do dados\ntokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sepra√ßao dos dados de treinos e testes\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    reviews_train_df_novo['text_pt_processed'].values,\n    reviews_train_df_novo['sentiment'].values,\n    test_size=0.2,\n    random_state=SEED\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convers√£o para lista de Strings\ntrain_texts = [str(text) for text in train_texts]  # Garante que s√£o strings\nval_texts = [str(text) for text in val_texts]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(train_texts))\nprint(type(train_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# classe para cria√ßao de um Dataset PyTorch permite otimizar o carregamento de dados e paralelizar a computa√ß√£o\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.encodings = tokenizer(\n            texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n        )\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx],\n            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n            \"labels\": self.labels[idx],\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Cria√ßao de dataset para treino e valida√ßao.\ntrain_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\nval_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Criacao de dataLoader para alimentar os modelos em lotes. O DataLoader permite carregar os dados em lotes (batch_size), tornando o treinamento mais eficiente\nbatch_size = 16\n\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset,\n    num_replicas=xm.xrt_world_size(),\n    rank=xm.get_ordinal(),\n    shuffle=True\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    sampler=train_sampler,\n    drop_last=True,\n    num_workers=4\n)\n\nval_sampler = torch.utils.data.distributed.DistributedSampler(\n    val_dataset,\n    num_replicas=xm.xrt_world_size(),\n    rank=xm.get_ordinal(),\n    shuffle=False\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    sampler=val_sampler,\n    drop_last=False,\n    num_workers=4\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carregamento do modelo pr√©-treinado BERT para classifica√ß√£o\nnum_labels = len(set(train_labels))  # N√∫mero de classes (geralmente 2 para sentimentos: positivo/negativo)\nmodel = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=num_labels).to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fun√ß√£o de perda para classifica√ß√£o\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Otimizador AdamW recomendado para transformers\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definindo √©pocas e Early stop\nnum_epochs = 5\nbest_val_loss = float(\"inf\")  # Melhor perda de valida√ß√£o inicial\npatience = 2  # Paci√™ncia para early stopping\n\n# Agendador de taxa de aprendizado ser√° definido dentro da fun√ß√£o de treinamento\n\ndef train_loop_fn(loader, model, optimizer, device):\n    num_training_steps = len(loader) * num_epochs\n    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n    \n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        loader.sampler.set_epoch(epoch)\n\n        for batch in loader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            \n            outputs = model(**inputs)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            loss.backward()\n            xm.optimizer_step(optimizer)\n            optimizer.zero_grad()\n            lr_scheduler.step()\n\n        avg_loss = xm.mesh_reduce('avg_train_loss', total_loss / len(loader), lambda x: sum(x) / len(x))\n        \n        if xm.is_master_ordinal():\n            print(f\"üîπ √âpoca {epoch+1}: Loss m√©dio: {avg_loss:.4f}\")\n\n        # Valida√ß√£o\n        model.eval()\n        total_val_loss = 0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**inputs)\n                total_val_loss += outputs.loss.item()\n\n        avg_val_loss = xm.mesh_reduce('avg_val_loss', total_val_loss / len(val_loader), lambda x: sum(x) / len(x))\n\n        if xm.is_master_ordinal():\n            print(f\"üî∏ √âpoca {epoch+1}: Loss de Valida√ß√£o: {avg_val_loss:.4f}\")\n\n            # Early Stopping\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                print(f\"‚ö†Ô∏è EarlyStopping: Nenhuma melhora na valida√ß√£o ({patience_counter}/{patience})\")\n\n            if patience_counter >= patience:\n                print(\"‚èπÔ∏è EarlyStopping ativado! Interrompendo o treinamento.\")\n                break\n\n# Fun√ß√£o principal para XLA\ndef _mp_fn(index):\n    model = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=num_labels).to(device)\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \ntrain_loop_fn(train_loader, model, optimizer, device)\n\n# Iniciar treinamento distribu√≠do\nxmp.spawn(_mp_fn, args=())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    predictions, true_labels = [], []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**inputs)\n            preds = torch.argmax(outputs.logits, dim=1)\n            \n            preds = xm.mesh_reduce('preds', preds.cpu().numpy(), lambda x: np.concatenate(x))\n            labels = xm.mesh_reduce('labels', inputs['labels'].cpu().numpy(), lambda x: np.concatenate(x))\n            \n            if xm.is_master_ordinal():\n                predictions.extend(preds)\n                true_labels.extend(labels)\n    \n    if xm.is_master_ordinal():\n        acc = accuracy_score(true_labels, predictions)\n        print(f\"üéØ Acur√°cia: {acc:.4f}\")\n    return acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Salvando Modelo","metadata":{}},{"cell_type":"code","source":"# Caminho onde o modelo ser√° salvo\nsave_path = \"/kaggle/working/imdb-ptbrmodelo_bert_treinado\"\n\n# Salvar modelo treinado\nmodel.save_pretrained(save_path)\n\n# Salvar o tokenizador tamb√©m (importante para carregar o modelo depois)\ntokenizer.save_pretrained(save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}